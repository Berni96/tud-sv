{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 - Linear Predictive Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; margin:5px 0px 0px 10px\" src=\"img/2-title.jpg\" width=\"500\">\n",
    "Das \"Linear Predictive Coding\" (LPC) ist ein Verfahren, das hauptsächlich in der Audiosignalverarbeitung und Sprachverarbeitung verwendet wird, um die Spektralhüllkurve eines digitalen Sprachsignals in komprimierter Form unter Verwendung der Informationen eines linearen Vorhersagemodells darzustellen. Es ist eine der leistungsstärksten Sprachanalysetechniken und eine der nützlichsten Methoden zum Codieren von Sprache in guter Qualität mit einer niedrigen Bitrate und liefert hochgenaue Schätzungen von Sprachparametern. LPC ist die am weitesten verbreitete Methode in der Sprachcodierung und Sprachsynthese.\n",
    "\n",
    "Thematisch wird LPC erst in Thema5-Hüllkurven behandelt, da sich mit LPC aber im Zuge der Bestimmung der Einhüllenden auch die Systemfunktion des z.B. Vokaltraktes (d.h. des dazugehörige _Synthesfilters_) schätzen lassen, wird sie in den Notebooks an dieser Stelle eingeführt. LPC unterscheidet sich aber grundlegend von dem Vorgehen der Messung der Übertragungsfunktion, wie es am Beispiel des Sweeps im Notebook 2.1 behandelt wurde.\n",
    "\n",
    "\n",
    "## Inhalt  \n",
    "<table style=\"width:256px; border: 1px solid black; display: inline-block\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:right\"><img src=\"img/2-1.png\" style=\"float:left\"></td>\n",
    "        <td style=\"text-align:left\" width=128px>\n",
    "            <a style=\"color:black; font-size:12px; font-weight:bold; text-decoration:none\" href='#1'>\n",
    "                1. Grundlagen\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td  style=\"text-align:right\" width=64px><img src=\"img/2-2.png\" style=\"float:left\"></td>\n",
    "        <td style=\"text-align:left\" width=256px>\n",
    "            <a style=\"color:black; font-size:12px; font-weight:bold; text-decoration:none\" href='#2'>\n",
    "                2. Anwendungsbeispiel\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "<div>\n",
    "    <img src=\"img/2-1.png\" style=\"float:left\">\n",
    "    <h2 style=\"position: relative; top: 6px; left: 6px\">\n",
    "        1. Grundlagen\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:top; margin:5px 0px 0px 10px\" src=\"img/Quelle-Filter-Modell.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die artikulatorische Sprachsynthese (anders als z.B. die datengestützte Sprachsynthese, wie sie in sämtlichen Sprachassistenten angewendet wird) basiert auf der sog. Quelle-Filter-Theorie, welche davon ausgeht, dass die Sprachproduktion in zwei Prozesse unterteilt werden kann, die unabhängig voneinander sind: die Erzeugung eines Anregungssignales und die anschließende Filterung dieses Anregungssignales. Unabhängig bedeutet in diesem Kontext, dass der nachgeschaltete Filterprozess das Anregungssignal nicht verändert, also keine Rückkopplung entsteht.\n",
    "\n",
    "Vereinfacht dargestellt erzeugen bei der Sprachproduktion die Glottis und die Stimmlippen unter Anregung eines Luftstromes das Anregungssignal, welches vom daraufffolgenden Vokaltrakt gefiltert wird. Das Anregungssignal kann dabei stimmhaft sein (d.h. es besitzt eine Grundperiode), stimmlos (Rauschrignal) oder eine Mischung aus Beidem. Wenn sich die Stimmlippen in einem vokalisierten Zustand befinden (Vibration der Stimmlippen), werden stimmhafte Laute (zum Beispiel Vokale) erzeugt. Wenn sich die Stimmbänder in einem \"stillen\" Zustand befinden, werden stimmlose Laute (zum Beispiel Konsonanten) erzeugt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LPC bietet die Möglichkeit, die Systemfunktion des Vokaltraktes zu schätzen, ohne das Anregungssignal zu kennen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grundidee:   \n",
    "Die Grundidee der LPC ist zunächst generell, dass die einzelnen Abtastwerte eines linear gefilerten Ausgangssignals $y(k)$ (in Falle der Sprachproduktion das abgestrahle Sprachschallsignal) nicht unabhängig voneinander sind. Stattdessen lässt sich jeder Abtastwert $y(k)$ aus einer Linearkombination endlich vielen vorangegangenen Abtastwerten annähern:\n",
    "\\begin{equation}\n",
    "\\hat{y}(k)=\\sum_{i=1}^{N}a_{i}y(k-i)\n",
    "\\end{equation}\n",
    "Die Gewichtungsfaktoren $a_i$ heißen _Prädiktorkoeffizienten_.\n",
    "\n",
    "Für jeden Abtastwert wird damit ein gewisser Prädiktionsfehler $e(k)$ gemacht:\n",
    "\\begin{equation}\n",
    "e(k)=y(k)-\\hat{y}(k)=y(k)-\\sum_{i=1}^{N}a_{i}y(k-i)\n",
    "\\end{equation}\n",
    "\n",
    "Die $a_{i}$ werden geschätzt, indem der mittlere quadratische Fehler (der sog. MSE, _mean squared error_) im betrachteten Signalabschnitt minimiert wird. \n",
    "\n",
    "Die Gleichung für $y(k)$ entspricht formal der Rekursionsgleichung für einen IIR-Allpol-Filter $H(z)$, wobei das Fehlersignal als (mit der Verstärkung $G$ skaliertes) Eingangssignal $x(k)$ betrachtet werden kann:\n",
    "\\begin{equation}\n",
    "y(k)=e(k)+\\sum_{i=1}^{N}a_{i}y(k-i)=Gx(k)+\\sum_{i=1}^{N}a_{i}y(k-i)\n",
    "\\end{equation}\n",
    "\n",
    "Die Systemfunktion ist somit\n",
    "\\begin{equation}\n",
    "H(z)=\\frac{Y(z)}{E(z)}=\\frac{1}{1-\\sum_{i=1}^{N}a_{i}z^{-i}}\n",
    "\\end{equation}\n",
    "\n",
    "Dieser Filter ist der sogenannte __LPC-Synthesefilter__. Der Amplitudengang des Synthesefilters entspricht bei einem weißen Anregungsspektrum der\n",
    "Hüllkurve im Frequenzbereich.\n",
    "\n",
    "Umgekehrt lässt sich das Fehlersignal $e(k)$ durch die Filterung von $y(k)$ mit dem inversen Filter $A(z) = 1/H(z)$ bestimmen. Dieser Filter wird als __Analysefilter__ bezeichnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:top; margin:5px 0px 0px 10px\" src=\"img/lpc.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "<div>\n",
    "    <img src=\"img/2-2.png\" style=\"float:left\">\n",
    "    <h2 style=\"position: relative; top: 6px; left: 6px\">\n",
    "        2.  Anwendungsbeispiel\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LPC findet, wie bereits erwähnt, breite Anwendung in der Spracherkennung, Sprachsynthese und Sprachkompression. Als Beispiel soll hier die lineare Prädiktion zur Formanterkennung verwendet werden, d.h., um die Formantfrequenzen (Resonanzfrequenzen) des Vokaltrakts zu schätzen. Im Quelle-Filter Sprachsignalmodell entsprechen die Formantfrequenzen den komplexen Polpaaren der Übertragungsfunktion. \n",
    "\n",
    "Ausgehend von einem kurzen Abschnitt des Signals werden die Prädiktorkoeffizienten geschätzt, welche wiederum die Filterkoeffizienten $a_i$ des Nennerpolynoms darstellen. Jede komplexe Wurzel der diskreten Systemfunktion des Synthesefilters lässt sich bekanntermaßen auch über \n",
    "\\begin{equation}\n",
    "p_{i}=r_{i}e^{j \\Omega_{i}},\\qquad r_i=\\sqrt{real(p_i)^2+imag(p_i)^2},\\qquad \\Omega_i = atan\\left(\\frac{imag(p_i)}{real(p_i)}\\right)\n",
    "\\end{equation}\n",
    "darstellen, d.h. die geschätzten Formantfrequenzen $f_i$ sind \n",
    "\\begin{equation}\n",
    "f_i = \\frac{\\Omega_i\\cdot f_s}{2\\pi},\n",
    "\\end{equation}\n",
    "Wobei nur die Frequenzen mit  positivem Vorzeichen in Frage kommen. Außerdem müssen die Formantfrequenzen überhalb der Grundfrequenz liegen, d.h. eine zusätzliche Bedingung ist $f_i >\\approx$ 90-100 Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren Sie zuerst die verwendeten externen Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import externer Module\n",
    "'''\n",
    "# ToDo: Importieren Sie:\n",
    "#    - numpy mit dem Alias np,\n",
    "#    - simpleaudio mit dem Alias sa,\n",
    "#    - Das Modul pyplot aus der Bibliothek matplotlib mit dem Alias plt,\n",
    "#    - scipy\n",
    "#    - Das Modul wavfile aus der Bibliothek scipy.io\n",
    "#    - librosa\n",
    "#    - Das Modul interact_manual aus der Bibliothek ipywidgets in den globalen Namenraum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import externer Module\n",
    "'''\n",
    "# Lösung\n",
    "\n",
    "import numpy as np\n",
    "import simpleaudio as sa\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import librosa\n",
    "from ipywidgets import interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun laden Sie das Audiosignal `akustik.wav` in das Projekt und bestimmen Sie dessen Eigenschaften:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audiosignal laden\n",
    "'''\n",
    "\n",
    "fs_Hz, audioSignal =  [..]          # ToDo: Laden Sie 'akustik.wav' mittels wavfile.read() in das Projekt\n",
    "audioSignal = [..]                  # ToDo: Normalisierung von audioSignal\n",
    "signalLength = [..]                 # ToDo: Länge des Audiosignals\n",
    "T_s = signalLength/fs_Hz - 1/fs_Hz  # Zeit\n",
    "t_s = [..]                          # ToDo: Erzeugen Sie ein Array mit den Abtastzeitpunkten mittels np.linspace() \n",
    "f_Hz = [..]                         # ToDo: Erzeugen Sie ein Array mit den Frequenzpunkten von [0-fs_Hz/2] mittels np.linspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audiosignal laden\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "fs_Hz, audioSignal = wavfile.read('data/akustik.wav')   # Sample Rate, Audiosignal im Array-Form\n",
    "audioSignal = audioSignal/np.max(np.abs(audioSignal))   # Normalisierung\n",
    "signalLength = len(audioSignal)                         # Länge des Audiosignals\n",
    "T_s = signalLength/fs_Hz - 1/fs_Hz                      # Zeit\n",
    "t_s = np.linspace(0, T_s, signalLength)                 # Zeitbereich\n",
    "f_Hz = np.linspace(0, fs_Hz/2, signalLength//2)         # Frequenzbereich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hören Sie sich nun das Array `audioSignal` an. Erstellen Sie dafür eine Funktion \"play_audio\", die mittels simpleaudio Arrays über die Lautsprecher ausgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion play_audio\n",
    "'''\n",
    "# ToDo: Erstellen sie eine Funktion, die das Eingangsarray in ein INT16 verwandelt und dann via simpleaudio ausgibt.\n",
    "def play_audio( [..] ):\n",
    "    [..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion play_audio\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "def play_audio(acoustic_signal):\n",
    "    sound = (acoustic_signal*(2**15-1)/np.max(np.abs(acoustic_signal))).astype(np.int16)\n",
    "    play_obj = sa.play_buffer(sound, 1, 2, int(fs_Hz))\n",
    "    play_obj.wait_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"play_audio\" an, um sich \"audioSignal\" anzuhören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "# Lösung\n",
    "play_audio(audioSignal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentieren Sie das Signal nun in ein 30 ms-großes Stück, indem der Vokal /a/ gesagt wird (Zum Beispiel zwischen 1,18-1,21 s), und berechnen Sie das Spektrum des /a/-Vokaltrakt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 30 ms langen /a/-Segments des Wortes „Akustik“\n",
    "''' \n",
    "# Zeitintervall bestimmen\n",
    "n_start =     [..]      # ToDo: Bestimmen Sie die Startsample-Nummer\n",
    "n_end =    [..]         # ToDo: Bestimmen Sie die Endsample-Nummer\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "t_aSegment_s = [..]  # ToDo: Segmentieren Sie das Zeitarray t_s auf das oben bestimmte Zeitintervall\n",
    "a_segment =  [..]  # ToDo: Segmentieren Sie das Signalarray audioSignal auf das oben bestimmte Zeitintervall               \n",
    "\n",
    "# Spektrum berechnen\n",
    "n_fft = 512  # Länge von FFT\n",
    "f_aSegment_Hz = np.linspace(0, fs_Hz/2, int(n_fft/2))   # Frequenzbereich\n",
    "aSegment_fft = [..]  # ToDo: wenden Sie eine FFT an mittels scipy.fftpack.fft()\n",
    "aSegment_fft_plot = np.abs(aSegment_fft[:len(f_aSegment_Hz)]) / int(n_fft/2)\n",
    "\n",
    "# Graphische Darstellung \n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal eines /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_aSegment_s, a_segment)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_aSegment_Hz, np.log(aSegment_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 30 ms langen /a/-Segments des Wortes „Akustik“\n",
    "''' \n",
    "# Lösung\n",
    "# Zeitintervall bestimmen\n",
    "n_start = int(1.18 / T_s * signalLength)                # Startsample-Nummer\n",
    "n_end = int(1.21 / T_s * signalLength)                  # Endsample-Nummer\n",
    "t_aSegment_s = t_s[n_start:n_end]\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "a_segment = audioSignal[n_start:n_end]               \n",
    "\n",
    "# Spektrum berechnen\n",
    "n_fft = 512; # Länge von FFT\n",
    "f_aSegment_Hz = np.linspace(0, fs_Hz-fs_Hz/2-fs_Hz/n_fft, int(n_fft/2))   # Frequenzbereich\n",
    "aSegment_fft = scipy.fftpack.fft(a_segment, n_fft)\n",
    "aSegment_fft_plot = np.abs(aSegment_fft[:len(f_aSegment_Hz)]) / int(n_fft/2)\n",
    "\n",
    "# Graphische Darstellung \n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal eines /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_aSegment_s, a_segment)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_aSegment_Hz, np.log(aSegment_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geben Sie sich nun das segmentierte Audiosignal `a_segment` aus. Da das Signal nur 30 ms lang ist, muss das Signal wiederholt werden. Definieren Sie deshalb dafür eine Funktion \"repeat_audio\", die das eingegebene Signal mehrfach hintereinander abspielt (erwarten sie dabei keine schön klingendes Signal. Es soll lediglich demonstriert werden, dass eine Tendenz hörbar ist, um welchen Vokal es sich handelt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion repeat_audio\n",
    "'''\n",
    "# ToDo: Erstellen sie eine Funktion, die das Eingangsarray in ein INT16 verwandelt, es mehrfach hintereinanderhängt und dann via simpleaudio ausgibt.\n",
    "def repeat_audio( [..] ):\n",
    "    [..]\n",
    "    for i in range(0, 7):\n",
    "        [..] = np.concatenate(( [..] ), axis = None)   # ToDo: eine Möglichkeit zum hintereinanderhängen des Signals\n",
    "    [..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion repeat_audio\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "def repeat_audio(acoustic_signal):\n",
    "    sound = (acoustic_signal*(2**15-1)/np.max(np.abs(acoustic_signal))).astype(np.int16)\n",
    "    for i in range(0, 7):\n",
    "        sound = np.concatenate((sound,sound), axis = None)\n",
    "    play_obj = sa.play_buffer(sound, 1, 2, int(fs_Hz))\n",
    "    play_obj.wait_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"repeat_audio\" an, um sich \"a_segment\" anzuhören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "# Lösung\n",
    "repeat_audio(signal.hann(a_segment.size)*a_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich können Sie das Spektrum eines 30 ms langen /u/-Segments des Wortes „Akustik“ berechnen (Zum Beispiel zwischen 1,42-1,25 s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 30 ms langen /u/-Segments des Wortes „Akustik“ berechnen\n",
    "''' \n",
    "# Zeitintervall bestimmen\n",
    "[..]\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "t_uSegment_s = [..]\n",
    "u_segment = [..]\n",
    "\n",
    "\n",
    "# Spektrum berechnen\n",
    "[..]\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal eines /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_uSegment_s, u_segment)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_uSegment_Hz, np.log(uSegment_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 30 ms langen /u/-Segments des Wortes „Akustik“ berechnen\n",
    "''' \n",
    "# Lösung\n",
    "# Zeitintervall bestimmen\n",
    "n_start = int(1.42 / T_s * signalLength)                # Startsample-Nummer\n",
    "n_end = int(1.45 / T_s * signalLength)                  # Endsample-Nummer\n",
    "t_uSegment_s = t_s[n_start:n_end]\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "u_segment = audioSignal[n_start:n_end]                  \n",
    "\n",
    "# Spektrum berechnen\n",
    "n_fft = 512     # Länge von FFT\n",
    "f_uSegment_Hz = np.linspace(0, fs_Hz-fs_Hz/2-fs_Hz/n_fft, int(n_fft/2))   # Frequenzbereich\n",
    "uSegment_fft = scipy.fftpack.fft(u_segment, n_fft)\n",
    "uSegment_fft_plot = np.abs(uSegment_fft[:len(f_uSegment_Hz)]) / int(n_fft/2)\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal eines /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_uSegment_s, u_segment)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_uSegment_Hz, np.log(uSegment_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geben Sie sich nun auch das segmentierte Signal `u_segment` aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"repeat_audio\" an, um sich \"u_segment\" anzuhören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "# Lösung\n",
    "repeat_audio(signal.hann(u_segment.size)*u_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir LP-Koeffizienten berechnen, um die Pole der Systemfunktion des Synthesefilters zu bestimmen. Dafür verwenden wir nun direkt eine Funktion des Moduls `librosa`, der LP-Koeffizienten sucht: [librosa.lpc(y, order)](https://librosa.org/doc/latest/generated/librosa.lpc.html)\n",
    "\n",
    "Dieser soll zunächst für das  /a/-Segment mit einer Filterordnung der Größe N = 12 betrachtet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Die LP-Koeffizienten des /a/-Segments berechnen und die Frequenzgang bzw. Polverteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "N = [..]           # ToDo: Legen Sie die Modellordnung des Synthesefilters fest\n",
    "b, a = [1], [..]   # ToDo: Bestimmen Sie die LP-Koeffizienten des Nenners über librosa.lpc()\n",
    "fH_Hz, H = [..]    # ToDo: Erzeugen Sie den Amplitudenfrequenzgang mit scipy.signal.freqz()\n",
    "z, p, k = [..]     # ToDo: Erzeugen Sie die Null-Pol Verteilung mi scipy.signal.tf2zpk()\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Amplitudenfrequenzgang des Synthesefilters für /a/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Frequenz [Hz]')\n",
    "plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Image')\n",
    "theta = np.arange(0, 2*np.pi, 0.01)\n",
    "plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "plt.plot(np.real(p), np.imag(p), 'x')\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "plt.gcf().set_size_inches(15, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Die LP-Koeffizienten des /a/-Segments berechnen und die Frequenzgang bzw. Polverteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "N = 12                                            # Modellordnung\n",
    "b, a = [1], librosa.lpc(a_segment, N)             # LP-Koeffizienten bestimmen\n",
    "fH_Hz, H = scipy.signal.freqz(b, a, fs=fs_Hz)     # Amplitudenfrequenzgang\n",
    "z, p, k = scipy.signal.tf2zpk(b, a)               # Null-Pol Verteilung\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Amplitudenfrequenzgang des Synthesefilters für /a/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Frequenz [Hz]')\n",
    "plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Image')\n",
    "theta = np.arange(0, 2*np.pi, 0.01)\n",
    "plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "plt.plot(np.real(p), np.imag(p), 'x')\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "plt.gcf().set_size_inches(15, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kritisch ist dabei die Wahl der „richtigen“ Modellordnung N. Die Unterschiede im Frequenzgang bei der Wahl unterschiedlicher Modellgrößern, soll nun für das /u/-Vokal-Segment genauer betrachtet werden. Erzeuegen Sie dafür mittels for-Schleife die Amplitudenfrequenzgänge bei Wahl der Synthesesfilterordnung auf 4., 8., 12., 20. und 24. Ordnung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aufgabe: Die LP-Koeffizienten des /u/-Segments mit verschiedenen Ordnungen berechnen und\n",
    "          die Frequenzgang bzw. Null-Pol Verteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "for N in ( [..] ):     # ToDo: Vorgabe der Filterordnungsgrößen  \n",
    "    b, a = [1], [..]   # ToDo: Bestimmen Sie die LP-Koeffizienten des Nenners über librosa.lpc()\n",
    "    fH_Hz, H = [..]    # ToDo: Erzeugen Sie den Amplitudenfrequenzgang mit scipy.signal.freqz()\n",
    "    z, p, k = [..]     # ToDo: Erzeugen Sie die Null-Pol Verteilung mi scipy.signal.tf2zpk()\n",
    "\n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(121)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    \n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aufgabe: Die LP-Koeffizienten des /u/-Segments mit verschiedenen Ordnungen berechnen und\n",
    "          die Frequenzgang bzw. Null-Pol Verteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "for N in (4, 8, 12, 20, 24):  \n",
    "    b, a = [1], librosa.lpc(u_segment, N)            # LP-Koeffizienten bestimmen\n",
    "    fH_Hz, H = scipy.signal.freqz(b, a, fs=fs_Hz)    # Amplitudenfrequenzgang\n",
    "    z, p, k = scipy.signal.tf2zpk(b, a)              # Null-Pol Verteilung\n",
    "\n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(121)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    plt.subplot(122)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um sich nun den Synthesefilter anzuhören, kann dieser invers Fouriertransformiert werden, um daraus die Impulsantwort zu erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Impulsantwort des modellierten /u/-Vokalfilters mit der Ordnung N = 24\n",
    "'''\n",
    "# Impulsantwort h des Modellfilters H\n",
    "h = [..]    # ToDo: implementieren Sie eine IFFT von H mittels scipy.fftpack.ifft()  mit einer Größe von 2 * len(H) [fftshift nicht vergessen]\n",
    "t_s = np.linspace(0, len(H)/fs_Hz, len(H))\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.title('Impulsantwort des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Zeit [s]')\n",
    "plt.ylabel('h [t]')\n",
    "\n",
    "plt.plot(t_s, np.real(h[int(len(H)):]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Impulsantwort des modellierten /u/-Vokalfilters mit der Ordnung N = 24\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "# Impulsantwort h des Modellfilters H\n",
    "h = np.fft.ifftshift(scipy.fftpack.ifft(H, 2*len(H)))\n",
    "t_s = np.linspace(0, len(H)/fs_Hz, len(H))\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.title('Impulsantwort des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Zeit [s]')\n",
    "plt.ylabel('h [t]')\n",
    "\n",
    "plt.plot(t_s, np.real(h[int(len(H)):]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erzeugen Sie nun ein Audiosignal, indem Sie die ersten 10 ms bzw. ~170 Samples (genauer [512:512+170]) mittels der \"repeat_audio\" abspielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"repeat_audio\" an, um sich die Impulsantwort \"h[512:512+170]\" anzuhören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "# Lösung\n",
    "repeat_audio(np.real(h[512:(512+170)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Schluss soll nun mittels `interact_manual` Änderungen in der Modellordnung  betrachtet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Interaktive LPC-Darstellung\n",
    "'''\n",
    "\n",
    "@interact_manual(N_i=(2, 50, 2),  segment_type_i=[('a-Segment', 1), ('u-Segment', 2)], output_sound_i = True)\n",
    "def interactive_linear_sweep(N_i=10, segment_type_i = 'a-Segment', output_sound_i=True):\n",
    "    if segment_type_i == 1:\n",
    "        segment = a_segment\n",
    "    elif segment_type_i == 2:\n",
    "        segment = u_segment\n",
    "    \n",
    "    # Berechnung der LP-Koeffizienten\n",
    "    b, a = [..]\n",
    "    \n",
    "    # Bestimmung des Amplitudenfrequenzgang und der Pol-Nullstellen-Verteilung\n",
    "    fH_Hz, H = [..]\n",
    "    z, p, k = [..]\n",
    "    \n",
    "    # Impulsantwort h des Modellfilters H\n",
    "    h = [..]\n",
    "    t_s = [..]\n",
    "    \n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(221)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    plt.subplot(222)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.subplot(212)\n",
    "    plt.title('Impulsantwort des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Zeit [s]')\n",
    "    plt.ylabel('h [t]')\n",
    "    plt.plot(t_s[0:170], np.real(h[int(len(H)):int(len(H))+170]))\n",
    "    plt.gcf().set_size_inches(20, 10)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Audioausgabe\n",
    "    if output_sound_i == True:   \n",
    "        repeat_audio(np.real(h[512:(512+170)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Interaktive LPC-Darstellung\n",
    "'''\n",
    "# Lösung\n",
    "\n",
    "@interact_manual(N_i=(2, 50, 2),  segment_type_i=[('a-Segment', 1), ('u-Segment', 2)], output_sound_i = True)\n",
    "def interactive_linear_sweep(N_i=10, segment_type_i = 'a-Segment', output_sound_i=True):\n",
    "    if segment_type_i == 1:\n",
    "        segment = a_segment\n",
    "    elif segment_type_i == 2:\n",
    "        segment = u_segment\n",
    "    \n",
    "    # LP-Koeffizienten \n",
    "    b, a = [1], librosa.lpc(segment, N_i)            # LP-Koeffizienten bestimmen\n",
    "    fH_Hz, H = scipy.signal.freqz(b, a, fs=fs_Hz)    # Amplitudenfrequenzgang\n",
    "    z, p, k = scipy.signal.tf2zpk(b, a)              # Null-Pol Verteilung\n",
    "    \n",
    "    # Impulsantwort h des Modellfilters H\n",
    "    h = np.fft.ifftshift(scipy.fftpack.ifft(H, 1024))\n",
    "    t_s = t_s = np.linspace(0, len(H)/fs_Hz, len(H))\n",
    "    \n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(221)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    plt.subplot(222)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.subplot(212)\n",
    "    plt.title('Impulsantwort des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Zeit [s]')\n",
    "    plt.ylabel('h [t]')\n",
    "    plt.plot(t_s[0:170], np.real(h[int(len(H)):int(len(H))+170]))\n",
    "    plt.gcf().set_size_inches(20, 10)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Audioausgabe\n",
    "    if output_sound_i == True:   \n",
    "        repeat_audio(np.real(h[512:(512+170)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Vorlesung ist die Richtlinie für die Wahl der Modellordnung: Es werden etwas mehr als doppelt soviele Prädiktorkoeffizienten benötigt, wie Formanten bis zur Nyquist-Frequenz erwartet werden. D.h., Modellordnung N ≈ Abtastfrequenz in kHz + 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Titelbild von [Justin Stoltzfus](https://www.dataversity.net/voice-processing-are-we-near-new-speech-recognition-apps/#), [Emflazie](https://en.wikipedia.org/wiki/Source%E2%80%93filter_model#/media/File:Source-filter_model_diagram.svg), [adaptivedigital](https://www.adaptivedigital.com/lpc/)   \n",
    "2. [Linear predictive coding](https://en.wikipedia.org/wiki/Linear_predictive_coding)\n",
    "3. [Introduction - Linear predictive coding](http://support.ircam.fr/docs/AudioSculpt/3.0/co/LPC.html)  \n",
    "---\n",
    "<div>Notebook erstellt von Arne-Lukas Fietkau, Yifei Li  und <a href=\"mailto:christoph.wagner@tu-dresden.de?Subject=Frage%20zu%20Jupyter%20Notebook%201.2%20IIR%20Filterentwurf\" target=\"_top\">Christoph Wagner</a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
