{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 - Linear Predictive Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; margin:5px 0px 0px 10px\" src=\"img/2-title.jpg\" width=\"500\">\n",
    "Das \"linear predictive coding\" (LPC) ist ein Verfahren, das hauptsächlich in der Audiosignalverarbeitung und Sprachverarbeitung verwendet wird, um die Spektralhüllkurve eines digitalen Sprachsignals in komprimierter Form unter Verwendung der Informationen eines linearen Vorhersagemodells darzustellen. Es ist eine der leistungsstärksten Sprachanalysetechniken und eine der nützlichsten Methoden zum Codieren von Sprache in guter Qualität mit einer niedrigen Bitrate und liefert hochgenaue Schätzungen von Sprachparametern. LPC ist die am weitesten verbreitete Methode in der Sprachcodierung und Sprachsynthese.\n",
    "\n",
    "In diesem Notebook wird dabei der Fokus auf der Erzeugung des Synthesefilters und die Anregung dessen.\n",
    "\n",
    "\n",
    "## Inhalt  \n",
    "<table style=\"width:256px; border: 1px solid black; display: inline-block\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:right\"><img src=\"img/2-1.png\" style=\"float:left\"></td>\n",
    "        <td style=\"text-align:left\" width=128px>\n",
    "            <a style=\"color:black; font-size:12px; font-weight:bold; text-decoration:none\" href='#1'>\n",
    "                1. Grundlagen\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td  style=\"text-align:right\" width=64px><img src=\"img/2-2.png\" style=\"float:left\"></td>\n",
    "        <td style=\"text-align:left\" width=256px>\n",
    "            <a style=\"color:black; font-size:12px; font-weight:bold; text-decoration:none\" href='#2'>\n",
    "                2. Anwendungsbeispiel\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "<div>\n",
    "    <img src=\"img/2-1.png\" style=\"float:left\">\n",
    "    <h2 style=\"position: relative; top: 6px; left: 6px\">\n",
    "        1. Grundlagen\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:top; margin:5px 0px 0px 10px\" src=\"img/Quelle-Filter-Modell.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Quell-Filter-Sprachsynthese basiert auf einer akustischen Theorie, die glaubt, dass Schall durch Anregung und entsprechende Filter gebildet wird.   \n",
    "\n",
    "Die Sprache wird von unserem Soundsystem erzeugt, das mit einer einfachen Schallquelle und einem Kanalmodell simuliert werden kann. Die Schallquelle wird von den Stimmbändern erzeugt. Die Stimmbänder liefern Erregungssignale für den Soundtrack. Diese Erregung kann periodisch oder nicht periodisch sein. Wenn sich die Stimmbänder in einem vokalisierten Zustand befinden (Vibration), werden Stimmgeräusche (zum Beispiel Vokale) erzeugt, wenn sich die Stimmbänder in einem stillen Zustand befinden, werden stille Geräusche (zum Beispiel Konsonanten) erzeugt. Der Kanal kann als Filter betrachtet werden, der das Spektrum des Anregungssignals von den Stimmbändern umformen kann, um verschiedene Klänge zu erzeugen. \n",
    "\n",
    "Bei der quellfilterbasierten Parametersynthese kann der Synthesizer-Workflow in drei Schritte unterteilt werden:\n",
    "\n",
    "- die entsprechende Glottalwellen-Anregungsquelle gemäß den tonalen Eigenschaften der zu synthetisierenden Silbe konstruieren  \n",
    "- ein neues Vokaltrakt-Parametermodell auf der Grundlage des ursprünglichen Vokaltrakts auf der Grundlage der Schalländerungsinformationen wie Koartikulation und Geschwindigkeitsänderung (Dauerparameter) erstellen  \n",
    "- die Glottalwellen-Anregungsquelle an das neue Vokaltraktmodell senden, die Ausgabe ist die synthetisierte Sprache, die die gegebenen prosodischen Eigenschaften erfüllt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LPC ist die auf diesem Modell basierende Sprachgenerierungstechnologie. In diesem Modell wird das Sprachsignal durch ein Anregungssignal durch ein zeitvariables Allpolfilter erzeugt. Der Koeffizient des Allpolfilters hängt von der Kanalform des spezifischen erzeugten Klangs ab. Das Anregungssignal ist entweder eine Impulsfolge von stimmhafter Sprache oder zufälliges Rauschen von leisem Ton. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grundidee:   \n",
    "Die Grundidee der LPC ist, der aktuelle Wert einer Sprachprobe kann durch eine gewichtete lineare Kombination der vergangenen Werte mehrerer Sprachproben angenähert werden. Die Gewichtungskoeffizienten in der linearen Kombination werden Prädiktor-Koeffizienten genannt. Indem die Summe der Quadrate der Differenzen zwischen den tatsächlichen Sprachabtastwerten und den linearen Vorhersageabtastwerten auf ein Minimum reduziert wird, kann ein eindeutiger Satz von Prädiktor-Koeffizienten bestimmt werden.\n",
    "\n",
    "Die Abtastwerte eines Signals $y(k)$, die durch lineare Filterung eines (unbekannten) Eingangssignals entstanden sind, sind nicht vollständig unabhängig voneinander. Statt dessen lässt sich jeder Abtastwert $y(k)$ aus endlich vielen vorangegangenen Abtastwerten durch folgende Prädiktorgleichung annähern:\n",
    "\\begin{equation}\n",
    "\\hat{y}(k)=\\sum_{i=1}^{N}a_{i}y(k-i)\n",
    "\\end{equation}\n",
    "\n",
    "Für jeden Abtastwert wird damit ein gewisser Prädiktionsfehler $e(k)$ gemacht:\n",
    "\\begin{equation}\n",
    "e(k)=y(k)-\\hat{y}(k)=y(k)-\\sum_{i=1}^{N}a_{i}y(k-i)\n",
    "\\end{equation}\n",
    "\n",
    "Die $a_{i}$ werden geschätzt, indem der mittlere quadratische Fehler im betrachteten Signalabschnitt minimiert wird.\n",
    "Bei der Analyse werden die Koeffizienten $a_{i}$ derart bestimmt, dass die Summe der Prädiktionsfehlerquadrate innerhalb eines endlichen Zeitfensters der Länge $L$ minimal wird.  \n",
    "\n",
    "Die Gleichung für $y(k)$ entspricht formal der Rekursionsgleichung für einen IIR-Allpol-Filter $H(z)$, wobei das Fehlersignal als (mit der Verstärkung $G$ skaliertes) Eingangssignal $x(k)$ betrachtet werden kann:\n",
    "\\begin{equation}\n",
    "y(k)=e(k)+\\sum_{i=1}^{N}a_{i}y(k-i)=Gx(k)+\\sum_{i=1}^{N}a_{i}y(k-i)\n",
    "\\end{equation}\n",
    "\n",
    "Die Systemfunktion ist somit\n",
    "\\begin{equation}\n",
    "H(z)=\\frac{Y(z)}{E(z)}=\\frac{1}{1-\\sum_{i=1}^{N}a_{i}z^{-i}}\n",
    "\\end{equation}\n",
    "\n",
    "Dieser Filter ist sogenannter __LPC-Synthesefilter__. Der Amplitudengang des Synthesefilters entspricht bei einem weißen Anregungsspektrum der\n",
    "Hüllkurve im Frequenzbereich.\n",
    "\n",
    "Umgekehrt lässt sich das Fehlersignal $e(k)$ durch die Filterung von $y(k)$ mit dem inversen Filter $A(z) = 1/H(z)$ bestimmen. Dieser Filter wird als __Analysefilter__ bezeichnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:top; margin:5px 0px 0px 10px\" src=\"img/lpc.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "<div>\n",
    "    <img src=\"img/2-2.png\" style=\"float:left\">\n",
    "    <h2 style=\"position: relative; top: 6px; left: 6px\">\n",
    "        2.  Anwendungsbeispiel\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LPC findet breite Anwendung in der Spracherkennung, Sprachsynthese und Sprachkompression. Als Beispiel hier verwenden wir die lineare Prädiktion zur Formanterkennung, d.h., um die Formantfrequenzen (Resonanzfrequenzen) des Vokaltrakts zu schätzen.\n",
    "\n",
    "Der Vokaltrakt kann als Schallröhre mit ungleichmäßigem Querschnitt betrachtet werden, die während des Schalls als Formant fungiert. Wenn der quasi-periodische Puls an der Stimmritze in den Vokaltrakt angeregt wird, verursacht er Resonanzeigenschaften und erzeugt einen Satz von Resonanzfrequenzen. Dieser Satz von Resonanzfrequenzen wird Resonanzspitzenfrequenzen oder einfach Formantenspitzen genannt.\n",
    "\n",
    "Im klassischen Sprachsignalmodell entspricht der Formante dem komplexen Polpaar der Übertragungsfunktion. Daher besteht der Schlüssel der Formantenschätzung darin, die Sprachspektralhüllkurve zu schätzen und zu berücksichtigen, dass der Maximalwert in der Spektralhüllkurve der Formante ist.\n",
    "\n",
    "Ausgehend von einem kurzen Abschnitt des Signals werden die Prädiktorkoeffizienten geschätzt. Diese bestimmen die Pole der Systemfunktion des Synthesefilters $H(z)=1/[1-\\sum_{i=1}^{N}a_{i}z^{-i}]$. Die komplexen Wurzeln des Nennerpolynoms können die Mittenfrequenz sowie Bandbreite der Formanten genau darstellen. Sei $z_{i}=r_{i}e^{j \\Omega_{i}}$ der beliebige komplexe Wurzel, dann ist die entsprechende Formantenfrequenz $ f_{i}= f_{s} \\Omega_{i} /2 \\pi $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren Sie zuerst die verwendeten externen Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import externer Module\n",
    "'''\n",
    "# ToDo: Importieren Sie:\n",
    "#    - numpy mit dem Alias np,\n",
    "#    - simpleaudio mit dem Alias sa,\n",
    "#    - Das Modul pyplot aus der Bibliothek matplotlib mit dem Alias plt,\n",
    "#    - scipy\n",
    "#    - Das Modul wavfile aus der Bibliothek scipy.io\n",
    "#    - librosa\n",
    "#    - Das Modul interact_manual aus der Bibliothek ipywidgets in den globalen Namenraum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import externer Module\n",
    "'''\n",
    "# Lösung\n",
    "\n",
    "import numpy as np\n",
    "import simpleaudio as sa\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "from ipywidgets import interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun laden Sie das Audiosignal `akustik.wav` in das Projekt und bestimmen Sie dessen Eigenschaften:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audiosignal laden\n",
    "'''\n",
    "\n",
    "fs_Hz, audioSignal =  [..]          # ToDo: Laden Sie 'akustik.wav' mittels wavfile.read() in das Projekt\n",
    "audioSignal = [..]                  # ToDo: Normalisierung von audioSignal\n",
    "signalLength = [..]                 # ToDo: Länge des Audiosignals\n",
    "T_s = signalLength/fs_Hz - 1/fs_Hz  # Zeit\n",
    "t_s = [..]                          # ToDo: Erzeugen Sie ein Array mit den Abtastzeitpunkten mittels np.linspace() \n",
    "f_Hz = [..]                         # ToDo: Erzeugen Sie ein Array mit den Frequenzpunkten von [0-fs_Hz/2] mittels np.linspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audiosignal laden\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "fs_Hz, audioSignal = wavfile.read('data/akustik.wav')   # Sample Rate, Audiosignal im Array-Form\n",
    "audioSignal = audioSignal/np.max(np.abs(audioSignal))   # Normalisierung\n",
    "signalLength = len(audioSignal)                         # Länge des Audiosignals\n",
    "T_s = signalLength/fs_Hz - 1/fs_Hz                      # Zeit\n",
    "t_s = np.linspace(0, T_s, signalLength)                 # Zeitbereich\n",
    "f_Hz = np.linspace(0, fs_Hz/2, signalLength//2)         # Frequenzbereich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hören Sie sich nun das Array `audioSignal` an. Erstellen Sie dafür eine Funktion \"play_audio\", die mittels simpleaudio Arrays über die Lautsprecher ausgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion play_audio\n",
    "'''\n",
    "# ToDo: Erstellen sie eine Funktion, die das Eingangsarray in ein INT16 verwandelt und dann via simpleaudio ausgibt.\n",
    "def play_audio( [..] ):\n",
    "    [..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion play_audio\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "def play_audio(acoustic_signal):\n",
    "    sound = (acoustic_signal*(2**15-1)/np.max(np.abs(acoustic_signal))).astype(np.int16)\n",
    "    play_obj = sa.play_buffer(sound, 1, 2, int(fs_Hz))\n",
    "    play_obj.wait_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"play_audio\" an, um sich \"audioSignal\" anzuhören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "# Lösung\n",
    "play_audio(audioSignal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentieren Sie das Signal nun in ein 30 ms-großes Stück, indem der Vokal /a/ gesagt wird (Zum Beispiel zwischen 1,18-1,21 s), und berechnen Sie das Spektrum des /a/-Vokaltrakt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 30 ms langen /a/-Segments des Wortes „Akustik“\n",
    "''' \n",
    "# Zeitintervall bestimmen\n",
    "n_start =     [..]      # ToDo: Bestimmen Sie die Startsample-Nummer\n",
    "n_end =    [..]         # ToDo: Bestimmen Sie die Endsample-Nummer\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "t_aSegment_s = [..]  # ToDo: Segmentieren Sie das Zeitarray t_s auf das oben bestimmte Zeitintervall\n",
    "a_segment =  [..]  # ToDo: Segmentieren Sie das Signalarray audioSignal auf das oben bestimmte Zeitintervall               \n",
    "\n",
    "# Spektrum berechnen\n",
    "n_fft = int(2**(np.floor(np.log2(len(a_segment)))+1))     # Länge von FFT\n",
    "f_aSegment_Hz = np.linspace(0, fs_Hz/2, int(n_fft/2))   # Frequenzbereich\n",
    "aSegment_fft = [..]  # ToDo: wenden Sie eine FFT an mittels scipy.fftpack.fft()\n",
    "aSegment_fft_plot = np.abs(aSegment_fft[:len(f_aSegment_Hz)]) / int(n_fft/2)\n",
    "\n",
    "# Graphische Darstellung \n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal eines /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_aSegment_s, a_segment)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_aSegment_Hz, np.log(aSegment_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 30 ms langen /a/-Segments des Wortes „Akustik“\n",
    "''' \n",
    "# Lösung\n",
    "# Zeitintervall bestimmen\n",
    "n_start = int(1.18 / T_s * signalLength)                # Startsample-Nummer\n",
    "n_end = int(1.21 / T_s * signalLength)                  # Endsample-Nummer\n",
    "t_aSegment_s = t_s[n_start:n_end]\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "a_segment = audioSignal[n_start:n_end]               \n",
    "\n",
    "# Spektrum berechnen\n",
    "n_fft = int(2**(np.floor(np.log2(len(a_segment)))+1))     # Länge von FFT\n",
    "f_aSegment_Hz = np.linspace(0, fs_Hz/2, int(n_fft/2))   # Frequenzbereich\n",
    "aSegment_fft = scipy.fftpack.fft(a_segment, n_fft)\n",
    "aSegment_fft_plot = np.abs(aSegment_fft[:len(f_aSegment_Hz)]) / int(n_fft/2)\n",
    "\n",
    "# Graphische Darstellung \n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal eines /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_aSegment_s, a_segment)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_aSegment_Hz, np.log(aSegment_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geben Sie sich nun das segmentierte Audiosignal `a_segment` aus. Da das Signal nur 30 ms lang ist, muss das Signal wiederholt werden. Definieren Sie deshalb dafür eine Funktion \"repeat_audio\", die das eingegebene Signal mehrfach hintereinander abspielt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion repeat_audio\n",
    "'''\n",
    "# ToDo: Erstellen sie eine Funktion, die das Eingangsarray in ein INT16 verwandelt, es mehrfach hintereinanderhängt und dann via simpleaudio ausgibt.\n",
    "def repeat_audio( [..] ):\n",
    "    [..]\n",
    "    for i in range(0, 7):\n",
    "        [..] = np.concatenate(( [..] ), axis = None)   # ToDo: eine Möglichkeit zum hintereinanderhängen des Signals\n",
    "    [..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion repeat_audio\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "def repeat_audio(acoustic_signal):\n",
    "    sound = (acoustic_signal*(2**15-1)/np.max(np.abs(acoustic_signal))).astype(np.int16)\n",
    "    for i in range(0, 7):\n",
    "        sound = np.concatenate((sound,sound), axis = None)\n",
    "    play_obj = sa.play_buffer(sound, 1, 2, int(fs_Hz))\n",
    "    play_obj.wait_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"repeat_audio\" an, um sich \"a_segment\" anzuhören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "# Lösung\n",
    "repeat_audio(a_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich können Sie das Spektrum eines 30 ms langen /u/-Segments des Wortes „Akustik“ berechnen (Zum Beispiel zwischen 1,42-1,25 s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 30 ms langen /u/-Segments des Wortes „Akustik“ berechnen\n",
    "''' \n",
    "# Zeitintervall bestimmen\n",
    "[..]\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "t_uSegment_s = [..]\n",
    "u_segment = [..]\n",
    "\n",
    "\n",
    "# Spektrum berechnen\n",
    "[..]\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal eines /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_uSegment_s, u_segment)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_uSegment_Hz, np.log(uSegment_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 30 ms langen /u/-Segments des Wortes „Akustik“ berechnen\n",
    "''' \n",
    "# Lösung\n",
    "# Zeitintervall bestimmen\n",
    "n_start = int(1.42 / T_s * signalLength)                # Startsample-Nummer\n",
    "n_end = int(1.45 / T_s * signalLength)                  # Endsample-Nummer\n",
    "t_uSegment_s = t_s[n_start:n_end]\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "u_segment = audioSignal[n_start:n_end]                  \n",
    "\n",
    "# Spektrum berechnen\n",
    "n_fft = int(2**(np.floor(np.log2(len(u_segment)))+1))     # Länge von FFT\n",
    "f_uSegment_Hz = np.linspace(0, fs_Hz/2, int(n_fft/2))   # Frequenzbereich\n",
    "uSegment_fft = scipy.fftpack.fft(u_segment, n_fft)\n",
    "uSegment_fft_plot = np.abs(uSegment_fft[:len(f_uSegment_Hz)]) / int(n_fft/2)\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal eines /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_uSegment_s, u_segment)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_uSegment_Hz, np.log(uSegment_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geben Sie sich nun auch das segmentierte Signal `u_segment` aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"repeat_audio\" an, um sich \"u_segment\" anzuhören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "# Lösung\n",
    "repeat_audio(u_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir LP-Koeffizienten berechnen, um die Pole der Systemfunktion des Synthesefilters zu bestimmen. Dafür verwenden wir nun direkt eine Funktion des Moduls `librosa`, der LP-Koeffizienten sucht: [librosa.lpc(y, order)](https://librosa.org/doc/latest/generated/librosa.lpc.html)\n",
    "\n",
    "Dieser soll zunächst für das  /a/-Segment mit einer Filterordnung der Größe N = 12 betrachtet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Die LP-Koeffizienten des /a/-Segments berechnen und die Frequenzgang bzw. Polverteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "N = [..]           # ToDo: Legen Sie die Modellordnung des Synthesefilters fest\n",
    "b, a = [1], [..]   # ToDo: Bestimmen Sie die LP-Koeffizienten des Nenners über librosa.lpc()\n",
    "fH_Hz, H = [..]    # ToDo: Erzeugen Sie den Amplitudenfrequenzgang mit scipy.signal.freqz()\n",
    "z, p, k = [..]     # ToDo: Erzeugen Sie die Null-Pol Verteilung mi scipy.signal.tf2zpk()\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Amplitudenfrequenzgang des Synthesefilters für /a/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Frequenz [Hz]')\n",
    "plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Image')\n",
    "theta = np.arange(0, 2*np.pi, 0.01)\n",
    "plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "plt.plot(np.real(p), np.imag(p), 'x')\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "plt.gcf().set_size_inches(15, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Die LP-Koeffizienten des /a/-Segments berechnen und die Frequenzgang bzw. Polverteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "N = 12                                            # Modellordnung\n",
    "b, a = [1], librosa.lpc(a_segment, N)             # LP-Koeffizienten bestimmen\n",
    "fH_Hz, H = scipy.signal.freqz(b, a, fs=fs_Hz)     # Amplitudenfrequenzgang\n",
    "z, p, k = scipy.signal.tf2zpk(b, a)               # Null-Pol Verteilung\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Amplitudenfrequenzgang des Synthesefilters für /a/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Frequenz [Hz]')\n",
    "plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Image')\n",
    "theta = np.arange(0, 2*np.pi, 0.01)\n",
    "plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "plt.plot(np.real(p), np.imag(p), 'x')\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "plt.gcf().set_size_inches(15, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kritisch ist dabei die Wahl der „richtigen“ Modellordnung N. Die Unterschiede im Frequenzgang bei der Wahl unterschiedlicher Modellgrößern, soll nun für das /u/-Vokal-Segment genauer betrachtet werden. Erzeuegen Sie dafür mittels for-Schleife die Amplitudenfrequenzgänge bei Wahl der Synthesesfilterordnung auf 4., 8., 12., 20. und 24. Ordnung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aufgabe: Die LP-Koeffizienten des /u/-Segments mit verschiedenen Ordnungen berechnen und\n",
    "          die Frequenzgang bzw. Null-Pol Verteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "for N in ( [..] ):     # ToDo: Vorgabe der Filterordnungsgrößen  \n",
    "    b, a = [1], [..]   # ToDo: Bestimmen Sie die LP-Koeffizienten des Nenners über librosa.lpc()\n",
    "    fH_Hz, H = [..]    # ToDo: Erzeugen Sie den Amplitudenfrequenzgang mit scipy.signal.freqz()\n",
    "    z, p, k = [..]     # ToDo: Erzeugen Sie die Null-Pol Verteilung mi scipy.signal.tf2zpk()\n",
    "\n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(121)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    \n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aufgabe: Die LP-Koeffizienten des /u/-Segments mit verschiedenen Ordnungen berechnen und\n",
    "          die Frequenzgang bzw. Null-Pol Verteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "for N in (4, 8, 12, 20, 24):  \n",
    "    b, a = [1], librosa.lpc(u_segment, N)            # LP-Koeffizienten bestimmen\n",
    "    fH_Hz, H = scipy.signal.freqz(b, a, fs=fs_Hz)    # Amplitudenfrequenzgang\n",
    "    z, p, k = scipy.signal.tf2zpk(b, a)              # Null-Pol Verteilung\n",
    "\n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(121)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    plt.subplot(122)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um sich nun den Synthesefilter anzuhören, kann dieser invers Fouriertransformiert werden, um daraus die Impulsantwort zu erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Impulsantwort des modellierten /u/-Vokalfilters mit der Ordnung N = 24\n",
    "'''\n",
    "# Impulsantwort h des Modellfilters H\n",
    "h = [..]    # ToDo: implementieren Sie eine IFFT von H mittels scipy.fftpack.ifft()  mit einer Größe von 2 * len(H) [fftshift nicht vergessen]\n",
    "t_s = np.linspace(0, len(H)/fs_Hz, len(H))\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.title('Impulsantwort des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Zeit [s]')\n",
    "plt.ylabel('h [t]')\n",
    "\n",
    "plt.plot(t_s, np.real(h[int(len(H)):]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Impulsantwort des modellierten /u/-Vokalfilters mit der Ordnung N = 24\n",
    "'''\n",
    "\n",
    "# Lösung\n",
    "# Impulsantwort h des Modellfilters H\n",
    "h = np.fft.ifftshift(scipy.fftpack.ifft(H, 2*len(H)))\n",
    "t_s = np.linspace(0, len(H)/fs_Hz, len(H))\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.title('Impulsantwort des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Zeit [s]')\n",
    "plt.ylabel('h [t]')\n",
    "\n",
    "plt.plot(t_s, np.real(h[int(len(H)):]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erzeugen Sie nun ein Audiosignal, indem Sie die ersten 10 ms bzw. ~170 Samples (genauer [512:512+170]) mittels der \"repeat_audio\" abspielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"repeat_audio\" an, um sich die Impulsantwort \"h[512:512+170]\" anzuhören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "# Lösung\n",
    "repeat_audio(np.real(h[512:(512+170)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Schluss soll nun mittels `interact_manual` Änderungen in der Modellordnung  betrachtet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Interaktive LPC-Darstellung\n",
    "'''\n",
    "\n",
    "@interact_manual(N_i=(2, 50, 2),  segment_type_i=[('a-Segment', 1), ('u-Segment', 2)], output_sound_i = True)\n",
    "def interactive_linear_sweep(N_i=10, segment_type_i = 'a-Segment', output_sound_i=True):\n",
    "    if segment_type_i == 1:\n",
    "        segment = a_segment\n",
    "    elif segment_type_i == 2:\n",
    "        segment = u_segment\n",
    "    \n",
    "    # Berechnung der LP-Koeffizienten\n",
    "    b, a = [..]\n",
    "    \n",
    "    # Bestimmung des Amplitudenfrequenzgang und der Pol-Nullstellen-Verteilung\n",
    "    fH_Hz, H = [..]\n",
    "    z, p, k = [..]\n",
    "    \n",
    "    # Impulsantwort h des Modellfilters H\n",
    "    h = [..]\n",
    "    t_s = [..]\n",
    "    \n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(221)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    plt.subplot(222)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.subplot(212)\n",
    "    plt.title('Impulsantwort des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Zeit [s]')\n",
    "    plt.ylabel('h [t]')\n",
    "    plt.plot(t_s[0:170], np.real(h[int(len(H)):int(len(H))+170]))\n",
    "    plt.gcf().set_size_inches(20, 10)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Audioausgabe\n",
    "    if output_sound_i == True:   \n",
    "        repeat_audio(np.real(h[512:(512+170)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Interaktive LPC-Darstellung\n",
    "'''\n",
    "# Lösung\n",
    "\n",
    "@interact_manual(N_i=(2, 50, 2),  segment_type_i=[('a-Segment', 1), ('u-Segment', 2)], output_sound_i = True)\n",
    "def interactive_linear_sweep(N_i=10, segment_type_i = 'a-Segment', output_sound_i=True):\n",
    "    if segment_type_i == 1:\n",
    "        segment = a_segment\n",
    "    elif segment_type_i == 2:\n",
    "        segment = u_segment\n",
    "    \n",
    "    # LP-Koeffizienten \n",
    "    b, a = [1], librosa.lpc(segment, N_i)            # LP-Koeffizienten bestimmen\n",
    "    fH_Hz, H = scipy.signal.freqz(b, a, fs=fs_Hz)    # Amplitudenfrequenzgang\n",
    "    z, p, k = scipy.signal.tf2zpk(b, a)              # Null-Pol Verteilung\n",
    "    \n",
    "    # Impulsantwort h des Modellfilters H\n",
    "    h = np.fft.ifftshift(scipy.fftpack.ifft(H, 1024))\n",
    "    t_s = t_s = np.linspace(0, len(H)/fs_Hz, len(H))\n",
    "    \n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(221)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    plt.subplot(222)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.subplot(212)\n",
    "    plt.title('Impulsantwort des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Zeit [s]')\n",
    "    plt.ylabel('h [t]')\n",
    "    plt.plot(t_s[0:170], np.real(h[int(len(H)):int(len(H))+170]))\n",
    "    plt.gcf().set_size_inches(20, 10)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Audioausgabe\n",
    "    if output_sound_i == True:   \n",
    "        repeat_audio(np.real(h[512:(512+170)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Vorlesung ist die Richtlinie für die Wahl der Modellordnung: Es werden etwas mehr als doppelt soviele Prädiktorkoeffizienten benötigt, wie Formanten bis zur Nyquist-Frequenz erwartet werden. D.h., Modellordnung N ≈ Abtastfrequenz in kHz + 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Titelbild von [Justin Stoltzfus](https://www.dataversity.net/voice-processing-are-we-near-new-speech-recognition-apps/#), [Emflazie](https://en.wikipedia.org/wiki/Source%E2%80%93filter_model#/media/File:Source-filter_model_diagram.svg), [adaptivedigital](https://www.adaptivedigital.com/lpc/)   \n",
    "2. [Linear predictive coding](https://en.wikipedia.org/wiki/Linear_predictive_coding)\n",
    "3. [Introduction - Linear predictive coding](http://support.ircam.fr/docs/AudioSculpt/3.0/co/LPC.html)  \n",
    "---\n",
    "<div>Notebook erstellt von Arne-Lukas Fietkau, Yifei Li  und <a href=\"mailto:christoph.wagner@tu-dresden.de?Subject=Frage%20zu%20Jupyter%20Notebook%201.2%20IIR%20Filterentwurf\" target=\"_top\">Christoph Wagner</a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
